{
  
    
        "post0": {
            "title": "Stochastic Real Business Cycle Model with Leisure and Many Shocks in Python",
            "content": "This notebook illustrates how to solve an extended version of the real business cycle model, where on top of the standard labor augmenting technology shocks, there are also shocks to the labor/leisure choice, the Euler equation and where government expenditures are stochastic. This is the prototype economy in Chari, Kehoe and McGrattan (2007) Business Cycle Accounting. . In the following, we use italics for scalars, lowercase bold for vectors and uppercase bold for matrices. . Model . Suppose households own the capital stock and rent it out at rate $r_t$. They also work for wages at rate $w_t$ per unit of labor input. The problem is to solve: . $$ max_{c_t,x_t,l_t} E_0 sum_{t=0}^ infty beta^t u(c_t,1-l_t)n_t$$ . subject to: . begin{align} c_t+(1+ tau_{xt})x_t&amp;=r_tk_t+(1- tau_{lt})w_tl_t+t_t n_{t+1}k_{t+1}&amp;=[(1- delta)k_t+x_t]n_t c_t&amp; geq 0, forall t end{align} Transfers are residually determined and made lump-sum after government expenditures have been incurred. Lowercase variables define per-capita quantitites, and $n_t$, is the population level at time $t$. . Firms are competitive and solve: . $$ max_{k_t,l_t} F(k_t, phi_tl_t)-r_tk_t-w_tl_t$$ . Finally, the resource constraint for the economy is given by: . $$c_t+x_t+g_t=y_t$$ . Suppose the following functional forms are used for preferences . $$U {c_t,l_t }_0^ infty = E_0 sum_{t=0}^ infty beta^t [ ln c_t+ psi ln(1-l_t)]n_t$$ . and technology . $$F(k_t, phi l_t)=k^ theta_t(z_t(1+g_z)^tl_t)^{1- theta}, ln z_t sim N(0,1).$$ . Functional Forms and Detrending . The utility function in detrended terms, is then given by . $$U { hat{c}_t,l_t }_0^ infty = E_0 sum_{t=0}^ infty beta^t [ ln hat{c}_t+ psi ln(1-l_t)+ ln(1+ gamma)^t]n_t$$ . where the notation for detrended variables follows $ hat{c}_t= frac{c_t}{(1+ gamma)^t}$ and we assume that $n_{t+1}=(1+g_n)n_t$. . The household budget constraint becomes . $$ hat{c}_t+(1+ tau_{xt}) hat{x}_t = r_t hat{k}_t+(1- tau_{lt}) hat{w}_tl_t+ hat{ phi}_t$$ . where $ hat{ phi}_t$ representes per-capita detrended government transfers. . The production function is also detrended and expressed in per-capita terms . $$ hat{y}= hat{k}^ theta_t(z_tl_t)^{1- theta}$$ . and consequently wages and the rental price of capital are the solution to the same problem . $$ max_{ hat{k}_t,l_t} hat{k}^ theta_t(z_tl_t)^{1- theta}-r_tk_t-w_tl_t$$ . Note also the capital accumulation equation in per-capita, detrended terms: . $$(1+n)(1+ gamma) hat{k}_{t+1}= hat{x}_t+(1- delta) hat{k}_t$$ . Optimization . The firms problem stated previously leads to the following first order conditions . begin{align} r_t &amp;= theta hat{k}_t^{ theta-1}(z_tl_t)^{1- theta} hat{w}_t &amp;=(1- theta)z_t hat{k}_t^ theta(z_tl_t)^{- theta} end{align} In order to set up the Lagrangian function for the representative household problem, I will first solve the capital accumulation equation for $ hat{x}_t$ . $$ hat{x}_t=(1+n)(1+ gamma) hat{k}_{t+1}-(1- delta) hat{k}_t$$ . and substitute it in the household budget constraint to get: . $$ hat{c}_t+(1+ tau_{xt})[(1+n)(1+ gamma) hat{k}_{t+1}-(1- delta) hat{k}_t] = r_t hat{k}_t+(1- tau_{lt}) hat{w}_tl_t+ hat{ phi}_t$$ . It is now time to set up the Lagrangian for the househehold problem. I now drop the $ ln (1+ gamma)^t$ from preferences since doing so, the preference ordering is not altered. . $$L^{HH}=E_0 sum_{t=0}^ infty beta^t left { [ ln{ hat{c}_t}+ psi ln(1-l_t)]n_t+ lambda_t left {r_t hat{k}_t+(1- tau_{lt}) hat{w}_tl_t+ hat{ phi}_t- hat{c}_t-(1+ tau_{xt})[(1+n)(1+ gamma) hat{k}_{t+1}-(1- delta) hat{k}_t] right } right }$$ . The Inada conditions are fulfilled for the above functional forms and assumptions and, together with the appropriate no-Maddoff and transversality conditions, the solution is defined by taking the necessary first order conditions w.r.t. consumption, capital and labor. . First Order Conditions . begin{align} hat{c}_t &amp;: frac{1}{ hat{c}_t}n_t= lambda_t l_t &amp;: frac{ psi}{1-l_t}n_t=(1- tau_{lt}) hat(w)_t lambda_t hat{k}_{t+1} &amp;: lambda_t(1+ tau_{xt}(1+n)(1+ gamma)= beta E_t lambda_{t+1}[r_{t+1}-(1+ tau_{xt})(1- delta)] end{align} From the firm&#39;s first order conditions, remember that $ hat{w}_t =(1- theta)z_t hat{k}_t^ theta(z_tl_t)^{- theta}$, so together with the first two equations, we get the labor-leisure equation . $$ frac{ psi hat{c}_t}{1-l_t}=(1- tau_{lt})(1- theta)z_t hat{k}_t^ theta(z_tl_t)^{- theta}$$ . The intertemporal condition or Euler equation reads . $$ frac{1}{ hat{c}_t}(1+ tau_{xt})= hat{ beta}E_t left { frac{1}{ hat{c}_{t+1}}[r_{t+1}-(1- tau_{x,t+1})(1- delta)] right }$$ . where I used the firm&#39;s f.o.c. w.r.t. capital and defined $ hat{ beta}= frac{ beta}{1+ gamma}$. . The model is closed and the solution implicitely defined by adding the households resource constraint to the set of equations defining the optimum . $$ hat{c}_t+(1+n)(1+ gamma) hat{k}_{t+1}-(1- delta) hat{k}_t + hat{g}_t= hat{y}_t$$ . Steady State Computation . In the steady state, the Euler equation is given by . $$(1+ tau_x)= hat{ beta}[r-(1- tau_x)(1- delta)]$$ . Solving this w.r.t. $r$ gives . $$r= frac{(1+ tau_x)[1- hat{ beta}(1- delta)]}{ hat{ beta}}$$ . Remember that $r= hat{k}^{ theta-1}(zl)^{1- theta}$ so that . begin{align} hat{k}= left { frac{(1+ tau_x)[1- hat{ beta}(1- delta)]}{ theta hat{ beta}} right }^{ frac{1}{ theta-1}}zl end{align} Let $A$ and $B$ be . begin{align} A &amp;= left { frac{z}{ frac{ hat{k}}{l}} right }^{1- theta} -(1- gamma)(1+n)+(1- delta) B &amp;= frac{(1- tau_l)(1- theta) left { frac{ hat{k}}{l} right }^ theta z^{1- theta}}{ psi} end{align} Then . begin{align} hat{k} &amp;= frac{B+g}{A+ frac{B}{ frac{ hat{k}}{l}}} hat{c} &amp;=A hat{k}-g l &amp;= frac{ hat{k}}{ frac{ hat{k}}{l}} hat{y} &amp;= hat{k}^ theta(zl)^{1- theta} hat{x} &amp;= hat{y}- hat{c}- hat{g} end{align} The stochastic nonlinear second order difference equation in capital . First we can rearrange the resource constraint to get consumption explicitely as a function of capital and labor i.e. $ hat{c}( hat{k}_t, hat{k}_{t+1},l_t, mathbf{s}_t)$. . Then,using this function with the intratemporal condition, we can define labor implicitely as a function of capital i.e. $l_t( hat{k}_t, hat{k}_{t+1}, mathbf{s}_t)$. . Finally, making use of these two functions, we can write the intertemporal condition as . $$ E_t left {F( hat{k}_{t+2}, hat{k}_{t+1}, hat{k}_t, mathbf{s_t}) right }=0$$ . This is the implicit formulation of the equilibrium solution for this economy. . We are going to approximate this non-linear stochastic second order difference equation with the log-linear specification . begin{equation*} E_t left {a_0 ln hat{k}_{t+2}+a_1 ln hat{k}_{t+1}+a_2 ln hat{k}_t+ mathbf{b}_0&#39; mathbf{s}_{t+1}+ mathbf{b}_1&#39; mathbf{s}_t right }=0 end{equation*} Log-Linear approximation to the policy functions . The aim now is to find a log-linear approximation to the solution of the form . begin{equation} ln hat{k}_{t+1}= gamma_k ln hat{k}_t+ mathbf{ gamma}&#39; mathbf{s}_t+ mathbf{ gamma}_0&#39; end{equation} where . begin{equation*} mathbf{s}_t = [ begin{matrix} ln z_t &amp; tau_{lt} &amp; tau_{xt} &amp; ln g_t] end{matrix} end{equation*} We assume . begin{equation*} mathbf{s}_{t+1}= mathbf{P} mathbf{s}_t+ mathbf{p}_0+ epsilon_{t+1} end{equation*} with $ epsilon_t sim N( mathbf{0}, mathbf{Q}&#39; mathbf{Q})$. . We now use these policy function approximations in the log-linear version of the approximation to the solution . begin{equation} E_t left { a_0( gamma_k ln hat{k}_{t+1}+ mathbf{ gamma}&#39; mathbf{s}_{t+1}) +a_1( gamma_k ln hat{k}_t+ mathbf{ gamma}&#39; mathbf{s}_t) +a_2 ln hat{k}_t + mathbf{b}&#39;_0 mathbf{s}_{t+1} + mathbf{b}&#39;_1 mathbf{s}_t right }=0 end{equation} The left-hand side can only be zero (in general) if $ gamma_k$ and $ mathbf{ gamma}$ satisfy the following system of equations . begin{equation*} cases{ a_0 gamma^2_k+a_1 gamma_k+a_2=0 cr a_0 gamma_k mathbf{ gamma} mathbf{P}+a_1 mathbf{ gamma}+ mathbf{b}&#39;_0 mathbf{P}+ mathbf{b}&#39;_1 = mathbf{0} } end{equation*} Because of the quadratic equation in $ gamma_k$, there will be two solutions for this system that are $1/ sqrt beta$ reciprocals. . The transversality condition imposes an upper bound for capital and therefore the solution to be chosen is the one associated with the root that is lower than $1 / sqrt beta$. Then . $ mathbf{ gamma}=-[(a_0 gamma_k+a_1) mathbf{I}_{4 times4}+a_0 mathbf{P}&#39;]^{-1}( mathbf{b}&#39;_0 mathbf{P}+ mathbf{b}&#39;_1 mathbf{I}_{4 times 4})&#39;$ . Once known the values of $ mathbf{ gamma}$ and $ gamma_k$, $ gamma_0$ is given by using the steady state values . $$ gamma_0=(1- gamma_k) ln hat{k}- mathbf{ gamma}&#39; mathbf{s}$$ . Exercise . Assume that one period is one quarter and the below parameters: . Parameter Value Description . $g_n$ | $1.015^{1/4}-1$ | Net population growth rate | . $g_z$ | $1.016^{1/4}-1$ | Net technology growth rate | . $ beta$ | $0.9722^{1/4}$ | Time preference | . $ delta$ | $1-(1-0.0464)^{1/4}$ | Capital depreciation rate | . $ psi$ | $2.24$ | Disutility of work | . $ sigma$ | $1.00001$ | CRRA coefficient | . $ theta$ | $0.35$ | Capital share of output | . Let $ mathbf{P}$ be: . $P = begin{bmatrix} 0.98 &amp;-0.014 &amp; -0.012 &amp; 0.192 -0.033 &amp; 0.956 &amp; -0.045 &amp; 0.057 -0.070 &amp;-0.046 &amp; 0.896 &amp; 0.104 0.005 &amp;-0.008 &amp; 0.049 &amp; 0.971 end{bmatrix} $ . Assume also that the unconditional mean of the $ mathbf{s}_t$ process $ mathbf{ overline{s}}$ is given by: . $ mathbf{ overline{s}} = begin{bmatrix} -0.024 &amp; 0.328 &amp; 0.483 &amp; -1.53 end{bmatrix}$ . Lastly, let the variance covariance matrix $ mathbf{Q}&#39; mathbf{Q}$ be such that: . $ Q = begin{bmatrix} 0.0116 &amp; 0 &amp; 0 &amp; 0 0.001 &amp; 0.956 &amp; -0.045 &amp; 0.057 -0.07 &amp;-0.046 &amp; 0.896 &amp; 0.104 0.005 &amp;-0.008 &amp; 0.049 &amp; 0.971 end{bmatrix} $ . Solve for the log-linear approximation to the capital policy function $ ln hat{k}_{t+1}= gamma_k ln hat{k}_t+ mathbf{ gamma}&#39; mathbf{s}_t+ mathbf{ gamma}_0&#39;$. Simulate time series for capital for 100 and 1000 realizations and plot the results. . Solution . Start by computing $ mathbf{p}_0$. Hint: $ mathbf{ overline{s}}= frac{ mathbf{p}_0}{ mathbf{I}- mathbf{P}}$. | Solve for the steady state level of capital $k_{ss}$. | Create a function that, for given parameters, maps $k_{t+2},k_{t+2},k_t, mathbf{s}_{t+1}$ and $ mathbf{s}_t$ to the residual of the Euler equation. | Use that function to compute $a_0, a_1, a_2, mathbf{b}_0$ and $ mathbf{b}_1$. Use numerical derivatives! | Given $a_0, a_1$ and $a_2$, find $ gamma_k$ | Given $a_0, gamma_k, a_1, mathbf{P}, mathbf{b}_0$ and $ mathbf{b}_1$, find $ mathbf{ gamma}$. | Given $ gamma_k, k_{ss}, mathbf{ gamma}$ and $ mathbf{ overline{s}}$, get $ gamma_0$. | Use the law of motion for $ mathbf{s_t}$ to get 1000 realizations. Assume $ mathbf{s}_0 = mathbf{ overline{s}}$. | Given the time series for $ mathbf{s}_t$, and the policy function for capital, get the time series for capital. Assume $k_1=k_{ss}$. | Plot the results using matplotlib. | Challenge: compute the IRF and plot them. |",
            "url": "https://jbduarte.github.io/blog/python/dynamic%20programming/2020/11/18/Stochastic-RBC-with-leisure.html",
            "relUrl": "/python/dynamic%20programming/2020/11/18/Stochastic-RBC-with-leisure.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FAVAR - BBE(2005) Replication in R",
            "content": "The FAVAR model is represented by the following system: . $$ begin{align} X_t &amp;= Lambda^f F_t + Lambda^y Y_t + e_t begin{bmatrix} F_t Y_t end{bmatrix} &amp;= Psi(L) begin{bmatrix} F_{t-1} Y_{t-1} end{bmatrix} + v_t end{align} $$where $F_t$ is the collection of variables that is unobservable, while $Y_t$ is is of the observable. For further description of the FAVAR model and its properties please see BBE (QJE, 2005) . library(readxl) library(boot) library(tsDyn) library(vars) library(repr) . # Get original large dataset of BBE (2005): 120 series data = read_excel(&quot;./Data/bbe_data.xlsx&quot;) . head(data) . DateIPPIPFIPCIPCDIPCNIPEIPIIPMIPMD...PU85PUCPUCDPUSPUXFPUXHSPUXMLEHCCLEHMHHSNTN . 1959:01 | 0.013396992 | 0.008609752 | 0.007316259 | 0.005227654 | 0.009517142 | 0.013284200 | 0.018846549 | 0.031175802 | 0.045049989 | ... | 0.004728141 | 0.000000000 | 0.000000000 | 0.004357305 | 0.000000000 | 0.000000000 | 0.000000000 | 0.003478264 | 0.004618946 | 95.8 | . 1959:02 | 0.006022800 | 0.004916504 | 0.000000000 | 0.019405634 | -0.004747249 | 0.010731414 | 0.013885375 | 0.025638933 | 0.038650015 | ... | 0.004705891 | -0.003007521 | 0.005235614 | 0.000000000 | 0.000000000 | 0.000000000 | 0.000000000 | -0.003478264 | 0.009174376 | 96.4 | . 1959:03 | 0.014342738 | 0.014547535 | 0.015653159 | 0.006396823 | 0.016505699 | 0.025817364 | 0.015959619 | 0.027254131 | 0.029650367 | ... | 0.000000000 | 0.000000000 | 0.000000000 | 0.004338402 | 0.003454235 | 0.000000000 | 0.000000000 | 0.006944472 | 0.004555816 | 96.9 | . 1959:04 | 0.008286750 | 0.009562755 | 0.004755444 | 0.020121006 | 0.000000000 | 0.031893735 | 0.005638694 | 0.025433102 | 0.034050126 | ... | 0.004683849 | 0.003007521 | 0.002607563 | 0.004319661 | 0.003442344 | 0.000000000 | 0.000000000 | 0.006896579 | 0.000000000 | 97.5 | . 1959:05 | 0.007036611 | 0.007141339 | -0.004755444 | 0.007458365 | -0.007033631 | 0.023250403 | 0.003368017 | -0.006587149 | -0.007362857 | ... | 0.004662013 | 0.000000000 | 0.000000000 | 0.000000000 | 0.003430535 | 0.003252035 | 0.003372685 | 0.003430535 | 0.004535155 | 97.2 | . 1959:06 | 0.001153906 | 0.008234277 | 0.013062217 | 0.019610667 | 0.008209100 | 0.002193842 | -0.013540671 | -0.061158362 | -0.105490860 | ... | 0.000000000 | 0.000000000 | 0.002600782 | 0.004301082 | 0.000000000 | 0.003241494 | 0.003361348 | 0.003418807 | 0.000000000 | 96.9 | . # Standardizing data = all variables with mean 0 and standard deviation 1. # This step is crucial in PC analysis data_s = scale(data[,2:121], center = TRUE, scale = TRUE) . Step 1: Extract principal componentes of all X (including Y) . pc_all = prcomp(data_s, center=FALSE, scale.=FALSE, rank. = 3) # summary(pc_all) C = pc_all$x # saving the principal components . Step 2: Extract principal componentes of Slow Variables . # Slow Variables slow_vars = c(&quot;IP&quot;, &quot;LHUR&quot;, &quot;PUNEW&quot;, &quot;IPP&quot;, &quot;IPF&quot;, &quot;IPC&quot;, &quot;IPCD&quot;, &quot;IPCN&quot;, &quot;IPE&quot;, &quot;IPI&quot;, &quot;IPM&quot;, &quot;IPMD&quot;, &quot;IPMND&quot;, &quot;IPMFG&quot;, &quot;IPD&quot;, &quot;IPN&quot;, &quot;IPMIN&quot;, &quot;IPUT&quot;, &quot;IPXMCA&quot;, &quot;PMI&quot;, &quot;PMP&quot;, &quot;GMPYQ&quot;, &quot;GMYXPQ&quot;, &quot;LHEL&quot;, &quot;LHELX&quot;, &quot;LHEM&quot;, &quot;LHNAG&quot;, &quot;LHU680&quot;, &quot;LHU5&quot;, &quot;LHU14&quot;, &quot;LHU15&quot;, &quot;LHU26&quot;, &quot;LPNAG&quot;, &quot;LP&quot;, &quot;LPGD&quot;, &quot;LPMI&quot;, &quot;LPCC&quot;, &quot;LPEM&quot;, &quot;LPED&quot;, &quot;LPEN&quot;, &quot;LPSP&quot;, &quot;LPTU&quot;, &quot;LPT&quot;, &quot;LPFR&quot;, &quot;LPS&quot;, &quot;LPGOV&quot;, &quot;LPHRM&quot;, &quot;LPMOSA&quot;, &quot;PMEMP&quot;, &quot;GMCQ&quot;, &quot;GMCDQ&quot;, &quot;GMCNQ&quot;, &quot;GMCSQ&quot;, &quot;GMCANQ&quot;, &quot;PWFSA&quot;, &quot;PWFCSA&quot;, &quot;PWIMSA&quot;, &quot;PWCMSA&quot;, &quot;PSM99Q&quot;, &quot;PU83&quot;, &quot;PU84&quot;, &quot;PU85&quot;, &quot;PUC&quot;, &quot;PUCD&quot;, &quot;PUS&quot;, &quot;PUXF&quot;, &quot;PUXHS&quot;, &quot;PUXM&quot;, &quot;LEHCC&quot;, &quot;LEHM&quot;) . data_slow = data_s[, slow_vars] pc_slow = prcomp(data_slow, center=FALSE, scale.=FALSE, rank. = 3) F_slow = pc_slow$x . Step 3: Clean the PC from the effect of observed Y . # Next clean the PC of all space from the observed Y reg = lm(C ~ F_slow + data_s[,&quot;FYFF&quot;]) #summary(reg) F_hat = C - data.matrix(data_s[,&quot;FYFF&quot;])%*%reg$coefficients[5,] # cleaning and saving F_hat . Step 4: Estimate FAVAR and get IRFs . . Warning: The IRFs in BBE are reported in standard deviation units, which means they are reported in the scaled data. No need to scale the data back to original units if want to compare with their paper. . data_var = data.frame(F_hat, &quot;FYFF&quot; = data_s[,&quot;FYFF&quot;]) var = VAR(data_var, p = 13) #summary(var) irf_point = irf(var, n.ahead = 48, impulse = &quot;FYFF&quot;, response = &quot;FYFF&quot;, boot = FALSE) # Shock size of 25 basis points impulse_sd = 0.25/sd(data$FYFF) scale = impulse_sd/(irf_point$irf$FYFF[1]) # position of FYFF response at step 0 # Computing Loading Factors reg_loadings = lm(data_s ~ 0 + F_hat + data_s[,&quot;FYFF&quot;]) loadings = reg_loadings$coefficients # head(reg_loadings$coefficients) #summary(reg_loadings) #### BOOTSTRAPING ######## R = 500 # Number of simulations nvars = 120 # Number of variables nsteps = 49 # numbers of steps IRFs = array(c(0,0,0), dim = c(nsteps,nvars,R)) var = lineVar(data_var, lag = 13, include = &quot;const&quot;) for(j in 1:R){ data_boot = VAR.boot(var, boot.scheme =&quot;resample&quot;) var_boot = VAR(data_boot, lag = 13) irf1 = irf(var_boot, n.ahead = 48, impulse = &quot;FYFF&quot;, boot = FALSE) for(i in 1:nvars){ IRFs[,i,j] = (irf1$irf$FYFF %*% matrix(loadings[1:4, i]))*scale } } ## Boot simulations done # Extract the quantiles of IRFs we are interested: 90% confidence intervals in BBE Upper = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ Upper[k,i] = quantile(IRFs[k,i,], probs = c(0.95))[1] } } Lower = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ Lower[k,i] = quantile(IRFs[k,i,], probs = c(0.05))[1] } } IRF = array(c(0,0), dim = c(nsteps, nvars)) for(k in 1:nsteps){ for(i in 1:nvars){ IRF[k,i] = quantile(IRFs[k,i,], probs = c(0.5))[1] } } rm(var_boot) rm(IRFs) . # Select the Variables you are Interested in # List of variables we are interested: FYFF, IP, CPI variables = c(grep(&quot;^FYFF$&quot;, colnames(data_s)), grep(&quot;^IP$&quot;, colnames(data_s)), grep(&quot;^PUNEW$&quot;, colnames(data_s)), grep(&quot;^FYGM3$&quot;, colnames(data_s)), grep(&quot;^FYGT5$&quot;, colnames(data_s)), grep(&quot;^FMFBA$&quot;, colnames(data_s)), grep(&quot;^FM2$&quot;, colnames(data_s)), grep(&quot;^EXRJAN$&quot;, colnames(data_s)), grep(&quot;^PMCP$&quot;, colnames(data_s)), grep(&quot;^IPXMCA$&quot;, colnames(data_s)), grep(&quot;^GMCQ$&quot;, colnames(data_s)), grep(&quot;^GMCDQ$&quot;, colnames(data_s)), grep(&quot;^GMCNQ$&quot;, colnames(data_s)), grep(&quot;^LHUR$&quot;, colnames(data_s)), grep(&quot;^PMEMP$&quot;, colnames(data_s)), grep(&quot;^LEHM$&quot;, colnames(data_s)), grep(&quot;^HSFR$&quot;, colnames(data_s)), grep(&quot;^PMNO$&quot;, colnames(data_s)), grep(&quot;^FSDXP$&quot;, colnames(data_s)), grep(&quot;^HHSNTN$&quot;, colnames(data_s)) ) transf_code = c(1, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 1, 1, 1, 1 ) variable_names = c(&quot;Fed Funds Rate&quot;, &quot;Industrial Production&quot;, &quot;CPI&quot;, &quot;3m Treasury Bills&quot;, &quot;5y Treasury Bonds&quot;, &quot;Monetary Base&quot;, &quot;M2&quot;, &quot;Exchange Rate Yen&quot;, &quot;Commodity Price Index&quot;, &quot;Capacity Util Rate&quot;, &quot;Personal Consumption&quot;, &quot;Durable Cons&quot;, &quot;Nondurable Cons&quot;, &quot;Unemployment&quot;, &quot;Employment&quot;, &quot;Avg Hourly Earnings&quot;, &quot;Housing Starts&quot;, &quot;New Orders&quot;, &quot;Dividends&quot;, &quot;Consumer Expectations&quot; ) . # Replicating Figure II in BBE (2005) - 3 Factors and Y = FYFF # Change plot size to 15 x 10 options(repr.plot.width=12, repr.plot.height=8) par(mfrow=c(5,4), mar = c(2, 2, 2, 2)) for(i in variables){ index = which(variables == i) if(transf_code[index] == 5){ plot(cumsum(IRF[,i]), type =&#39;l&#39;,lwd=2, main = variable_names[index], ylab= &quot;&quot;, xlab=&quot;Steps&quot;, ylim=range(cumsum(Lower[,i]),cumsum(Upper[,i])), cex.main=1.8, cex.axis=1.3) lines(cumsum(Upper[,i]), lty=2, col=&quot;red&quot;) lines(cumsum(Lower[,i]), lty=2, col=&quot;red&quot;) abline(h=0) } else{ plot(IRF[,i], type =&#39;l&#39;,lwd=2, main = variable_names[index], ylab= &quot;&quot;, xlab=&quot;Steps&quot;, ylim=range((Lower[,i]),(Upper[,i])), cex.main=1.8, cex.axis=1.3) lines((Upper[,i]), lty=2, col=&quot;red&quot;) lines((Lower[,i]), lty=2, col=&quot;red&quot;) abline(h=0) } } . How much of the variation is captured by the Factors and FYFF? . options(repr.plot.width=12, repr.plot.height=6) par(mfrow=c(1,2), mar = c(2, 2, 2, 2)) plot(data_s[, variables[2]], type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;Industrial Production&quot;) lines(fitted(reg_loadings)[,variables[2]], lty=2, col=&quot;red&quot;) legend(300,6, legend=c(&quot;Data&quot;, &quot;Predicted by PC&quot;), lty = 1:2, col = c(&quot;black&quot;, &quot;red&quot;), box.lty=0) plot(data_s[, variables[3]], type = &quot;l&quot;, ylab = &quot;&quot;, main = &quot;CPI&quot;) lines(fitted(reg_loadings)[,variables[3]], lty=2, col=&quot;red&quot;) . Step 5: FEVD . One step-ahead Variance Decomposition of $X_t$: . $$ E_t[X_{t+1} - X_t] = Lambda^f E_t[F_{t+1} - F_t] + Lambda^y E_t [Y_{t+1} - Y_t] + e_{t+1} $$$$ F_t = mu_f + L^1 varepsilon_t + A_1 L^1 varepsilon_t + ... $$$$ Y_t = mu_y + L^2 varepsilon_t + A_1 L^2 varepsilon_t + ... $$ where $L = begin{bmatrix} L^1 L^2 end{bmatrix}$ is the lower triangle matrix from the Cholesky decomposition. Let $ Lambda = begin{bmatrix} Lambda^f Lambda^y end{bmatrix}$, we have that: . $$ E_t[X_{t+1} - X_t] = Lambda L E_t[ varepsilon_{t+1}] + E_t[e_{t+1}] $$$$ E_t[X_{t+1} - X_t] = Psi(0) E_t[ varepsilon_{t+1}] + E_t[e_{t+1}] $$$$ var(E_t[X_{t+1} - X_t]) = Psi(0) Sigma_ varepsilon ( Psi(0)))&#39; + sigma^2_e $$where $ Sigma_ varepsilon = I $ because of the Cholesky decomposition, and $ Psi(0)$ is a 1x4 vector with the contemporaneous response of X_t to all structural shocks. Hence, the total variance of each observable variable $X_t$ is just the sum of the squared impulse response functions plus the variance of the measurement error. . At step-ahead $j$: . $$ var(E_t[X_{t+j} - X_t]) = sum_{i=0}^j Psi(i)( Psi(i)))&#39; + sigma^2_e $$ The variance of $E_t[X_{t+j} - X_t]$ that comes from the monetary policy shock alone is given by: . $$ var(E_t[X_{t+j} - X_t| varepsilon_F, e_t]) = sum_{i=0}^j Psi_{MP}(i)( Psi(i)_{MP}))&#39; $$where $ Psi(i)_{MP} = Psi(i)(1,4)$, i.e. the impulse response to monetary policy shock. Hence, the contribution of monetary policy shock to the $j$ step-ahead forecast error variance is given by: . $$ { text{FEVD}}(j)_{MP}^X = frac{var(E_t[X_{t+j} - X_t| varepsilon_F, e_t])}{var(E_t[X_{t+j} - X_t])} $$ # Get the VAR point estimates hor = 60 var = VAR(data_var, p = 13) irf_point = irf(var, n.ahead = hor, boot = FALSE) . # Get IRFs for all of X we are interested in, Dimensions: (hor, key_nvars) # Find loadings results = summary(reg_loadings) # the warning comes because of FYFF key_nvars = length(variables) irf_X_pc1 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_pc2 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_pc3 = array(c(0,0), dim=c(hor+1, key_nvars)) irf_X_fyff = array(c(0,0), dim=c(hor+1, key_nvars)) for(i in 1:key_nvars){ irf_X_pc1[,i] = irf_point$irf$PC1 %*% matrix(loadings[1:4, variables[i]]) irf_X_pc2[,i] = irf_point$irf$PC2 %*% matrix(loadings[1:4, variables[i]]) irf_X_pc3[,i] = irf_point$irf$PC3 %*% matrix(loadings[1:4, variables[i]]) irf_X_fyff[,i] = (irf_point$irf$FYFF) %*% matrix(loadings[1:4, variables[i]]) } . Warning message in summary.lm(object, ...): “essentially perfect fit: summary may be unreliable” . # Get the IRFs squared and accumulate them psi2_pc1 = array(0, dim = key_nvars) psi2_pc2 = array(0, dim = key_nvars) psi2_pc3 = array(0, dim = key_nvars) psi2_fyff = array(0, dim = key_nvars) for(i in 1:key_nvars){ for(j in 1:hor){ psi2_pc1[i] = psi2_pc1[i] + irf_X_pc1[j,i]^2 psi2_pc2[i] = psi2_pc2[i] + irf_X_pc2[j,i]^2 psi2_pc3[i] = psi2_pc3[i] + irf_X_pc3[j,i]^2 psi2_fyff[i] = psi2_fyff[i] + irf_X_fyff[j,i]^2 } } . var_total= array(0, dim = key_nvars) var_fac= array(0, dim = key_nvars) var_e= array(0, dim = key_nvars) for(i in 1:key_nvars){ var_fac[i] = psi2_pc1[i] + psi2_pc2[i] + psi2_pc3[i] + psi2_fyff[i] var_total[i] = psi2_pc1[i] + psi2_pc2[i] + psi2_pc3[i] + psi2_fyff[i] + results[[variables[i]]]$sigma^2 var_e[i] = results[[variables[i]]]$sigma^2 } . table = data.frame(&quot;PC1&quot; = round((psi2_pc1),3), &quot;PC2&quot; = round((psi2_pc2),3), &quot;PC3&quot; = round((psi2_pc3),3), &quot;FYFF&quot; = round((psi2_fyff),3), &quot;Factor_Y_total&quot; = round(var_fac,3) ,&quot;e&quot; = round((var_e),3), &quot;Total&quot; = round(var_total,3)) row.names(table) = variable_names table . PC1PC2PC3FYFFFactor_Y_totaleTotal . Fed Funds Rate0.497 | 0.279 | 0.059 | 0.150 | 0.985 | 0.000 | 0.985 | . Industrial Production0.550 | 0.037 | 0.054 | 0.147 | 0.787 | 0.255 | 1.042 | . CPI0.155 | 0.701 | 0.019 | 0.117 | 0.991 | 0.138 | 1.129 | . 3m Treasury Bills0.456 | 0.281 | 0.048 | 0.130 | 0.914 | 0.025 | 0.939 | . 5y Treasury Bonds0.299 | 0.309 | 0.015 | 0.087 | 0.710 | 0.068 | 0.778 | . Monetary Base0.017 | 0.056 | 0.010 | 0.005 | 0.087 | 0.900 | 0.988 | . M20.017 | 0.014 | 0.007 | 0.005 | 0.042 | 0.953 | 0.995 | . Exchange Rate Yen0.016 | 0.003 | 0.005 | 0.005 | 0.028 | 0.981 | 1.010 | . Commodity Price Index0.250 | 0.272 | 0.032 | 0.154 | 0.709 | 0.362 | 1.071 | . Capacity Util Rate0.365 | 0.069 | 0.057 | 0.146 | 0.637 | 0.252 | 0.889 | . Personal Consumption0.057 | 0.044 | 0.004 | 0.017 | 0.122 | 0.895 | 1.017 | . Durable Cons0.039 | 0.013 | 0.004 | 0.012 | 0.068 | 0.942 | 1.010 | . Nondurable Cons0.030 | 0.029 | 0.002 | 0.008 | 0.069 | 0.943 | 1.012 | . Unemployment0.251 | 0.176 | 0.064 | 0.088 | 0.578 | 0.182 | 0.760 | . Employment0.451 | 0.062 | 0.035 | 0.164 | 0.712 | 0.277 | 0.990 | . Avg Hourly Earnings0.040 | 0.160 | 0.007 | 0.026 | 0.232 | 0.792 | 1.025 | . Housing Starts0.280 | 0.033 | 0.023 | 0.093 | 0.428 | 0.600 | 1.028 | . New Orders0.437 | 0.055 | 0.044 | 0.156 | 0.691 | 0.350 | 1.041 | . Dividends0.109 | 0.346 | 0.007 | 0.040 | 0.502 | 0.448 | 0.950 | . Consumer Expectations0.173 | 0.462 | 0.014 | 0.061 | 0.710 | 0.290 | 1.000 | . # Replicating Table I in BBE (2005) - 3 Factors and Y = FYFF r2 = array(0, dim = key_nvars) for(i in 1:key_nvars){ r2[i] = results[[variables[i]]]$r.squared } . table2 = data.frame(&quot;Variables&quot; = variable_names, &quot;Contribution&quot; = round((psi2_fyff/var_total),3), &quot;R-squared&quot; = round(r2,3)) table2 . VariablesContributionR.squared . Fed Funds Rate | 0.152 | 1.000 | . Industrial Production | 0.141 | 0.746 | . CPI | 0.104 | 0.863 | . 3m Treasury Bills | 0.139 | 0.975 | . 5y Treasury Bonds | 0.112 | 0.933 | . Monetary Base | 0.005 | 0.105 | . M2 | 0.005 | 0.052 | . Exchange Rate Yen | 0.005 | 0.024 | . Commodity Price Index | 0.144 | 0.640 | . Capacity Util Rate | 0.165 | 0.749 | . Personal Consumption | 0.017 | 0.110 | . Durable Cons | 0.012 | 0.063 | . Nondurable Cons | 0.008 | 0.063 | . Unemployment | 0.116 | 0.819 | . Employment | 0.166 | 0.724 | . Avg Hourly Earnings | 0.025 | 0.212 | . Housing Starts | 0.090 | 0.404 | . New Orders | 0.150 | 0.652 | . Dividends | 0.042 | 0.555 | . Consumer Expectations | 0.061 | 0.712 | .",
            "url": "https://jbduarte.github.io/blog/time%20series/r/favar/2020/04/24/FAVAR-Replication.html",
            "relUrl": "/time%20series/r/favar/2020/04/24/FAVAR-Replication.html",
            "date": " • Apr 24, 2020"
        }
        
    
  

  
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jbduarte.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}